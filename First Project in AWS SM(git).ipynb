{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22614034",
   "metadata": {},
   "source": [
    "# Importing Important Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1aaaa",
   "metadata": {},
   "source": [
    "### Steps to be followed\n",
    "\n",
    "1) Importing necessary libraries \n",
    "2) Creating S3 Bucket\n",
    "3) Mapping Train and Test data in S3\n",
    "4) Mapping the path of model in S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fee4407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker #we are using sager because in this project we are going to use the inbuild algorithms which are present in AWS Sagemaker(eg : - XGBoost)\n",
    "import boto3 #whenever working with sage make you've to import boto3 by using boto3 we can also read s3 bucket from local python environment if it is public\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri #we'll be downloading an image container which has the whole xgboost inbuilt algorithm that whole thing can be download using that get_image_uri library\n",
    "from sagemaker.session import s3_input, Session #ss3_input and Session are the functions we are importing to configure input data channels from Amazon S3 (Simple Storage Service) for training a machine learning model on SageMaker and to interact with various SageMaker resources, such as creating training jobs, deploying models, managing endpoints, etc\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601dc55",
   "metadata": {},
   "source": [
    "#### Creating a S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb23fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'projectbucket189' #assigning the name of the bucket\n",
    "my_region = boto3.session.Session().region_name #setting the region of the instance\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e9019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "# Now creating an S3 bucket with help of code\n",
    "\n",
    "# Initialize an S3 resource\n",
    "s3 = boto3.resource(\"s3\")\n",
    "# Initialize an S3 client with a specific region\n",
    "s3 = boto3.client('s3', region_name='us-east-1')\n",
    "try: \n",
    "     # Check if the specified region is 'us-east-1'\n",
    "    if my_region == 'us-east-1':\n",
    "        # Attempt to create an S3 bucket\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    # If no exception occurred, print success message\n",
    "    print('S3 Bucket created successfully')\n",
    "except Exception as e :\n",
    "    # If an exception occurred during bucket creation, print the error message\n",
    "    print('s3 error: ', e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc95f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://projectbucket189/xgboost-as-a-built-in-algo/output\n"
     ]
    }
   ],
   "source": [
    "# Setting an output path where our model will be saved once we train it \n",
    "\n",
    "# using the sagemaker we are using the xgboost built in alog\n",
    "prefix = 'xgboost-as-a-built-in-algo'\n",
    "#using string formatting we are setting our output path in place of those 2 {}'s in first{} our bucket name will be placed and in the second {} our prefix which is xgboost algo\n",
    "output_path= 's3://{}/{}/output'.format(bucket_name, prefix) \n",
    "print(output_path)\n",
    "# In the output we can see our output path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90b360",
   "metadata": {},
   "source": [
    "### Downloading the dataset and storing it in our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5254aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded bank_clean.csv.\n",
      "Success : Data loaded into dataframe. \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# with the help of url lib we are downloading out dataset \n",
    "import urllib\n",
    "try:\n",
    "    urllib.request.urlretrieve (\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\",\"bank_clean.csv\")\n",
    "    print('Success: downloaded bank_clean.csv.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ' ,e)\n",
    "\n",
    "try:\n",
    "#     and loading the dataset into a variable name model_data\n",
    "    model_data = pd.read_csv('./bank_clean.csv',index_col= 0)\n",
    "    print('Success : Data loaded into dataframe. ')\n",
    "except Exception as e:\n",
    "    print('Data load error: ', e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c1a89",
   "metadata": {},
   "source": [
    "### Dataset details :- Our Dataset is basically a cleaned dataset which states weather the customer buys the product or not (bascially it is onehot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ccc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e24592fb",
   "metadata": {},
   "source": [
    "### Train & test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d9a59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 61) (12357, 61)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "# train test split is not done as usual like we do xtrain, ytrain, xtest, ytest basically who data is divided into train_data and test_data so both of the train and test data will be consisting both dependent and independent variables \n",
    "train_data, test_data = np.split(model_data.sample(frac=1, random_state = 1729), [int(0.7* len(model_data))])\n",
    "# getting the shapes of tain and test data\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3994448",
   "metadata": {},
   "source": [
    "#### Remember :whenever we are dealing with sagemaker the dependent feature should be your first column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b796f7",
   "metadata": {},
   "source": [
    "### Creating & Saving seperate Train and test data into s3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42d4b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "import os\n",
    "# from y_no and y_yes dependent feature we are taking y_yes feature and concatinating all other features and creating into a csv file\n",
    "pd.concat([train_data['y_yes'],train_data.drop(['y_no','y_yes'],axis =1)], axis =1).to_csv('train.csv',index =False , header = False) \n",
    "# uploading that train.csv file into s3 bucket\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "# Whenever we're training our model the data path is given from the s3 bucket for that we've to create the path\n",
    "# inorder to create the path we are using an inbuilt function of sagemaker which is sagemaker.TrainingInput \n",
    "# then we are giving the whole path of the training folder just like last time we did in setting output path \n",
    "# and the content type will be csv becoz the whole which we're uploading is a csv file\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data = 's3://{}/{}/train'.format(bucket_name, prefix), content_type ='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c1effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "# doing the same for the test data as well\n",
    "import os\n",
    "pd.concat([test_data['y_yes'],test_data.drop(['y_no','y_yes'],axis =1)], axis =1).to_csv('test.csv',index =False , header = False) \n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "s3_input_test = sagemaker.TrainingInput(s3_data = 's3://{}/{}/test'.format(bucket_name, prefix), content_type ='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520eee73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
